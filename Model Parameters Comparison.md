# UFC Predictor Models - Parameter Comparison

## **üìä MODEL PARAMETERS COMPARISON TABLE**

| **Parameter** | **exe Base** | **Two Runs** | **Class Weighting** | **Data Augmentation** |
|---------------|--------------|--------------|---------------------|----------------------|
| **üéØ XGBOOST PARAMETERS** | | | | | |
| **n_estimators** | 800 | 800 | 600 | 800 |
| **max_depth** | 10 | 10 | 8 | 10 |
| **learning_rate** | 0.015 | 0.015 | 0.02 | 0.015 |
| **subsample** | 0.85 | 0.85 | 0.8 | 0.85 |
| **colsample_bytree** | 0.85 | 0.85 | 0.8 | 0.85 |
| **reg_alpha** | 0.1 | 0.1 | 0.2 | 0.1 |
| **reg_lambda** | 0.8 | 0.8 | 1.0 | 0.8 |
| **min_child_weight** | 3 | 3 | 4 | 3 |
| **gamma** | 0.15 | 0.15 | 0.15 | 0.15 |
| **üéØ LIGHTGBM PARAMETERS** | | | | | |
| **n_estimators** | 800 | 800 | 600 | 800 |
| **max_depth** | 10 | 10 | 8 | 10 |
| **learning_rate** | 0.015 | 0.015 | 0.02 | 0.015 |
| **num_leaves** | 80 | 80 | 60 | 80 |
| **subsample** | 0.85 | 0.85 | 0.8 | 0.85 |
| **colsample_bytree** | 0.85 | 0.85 | 0.8 | 0.85 |
| **reg_alpha** | 0.1 | 0.1 | 0.2 | 0.1 |
| **reg_lambda** | 0.8 | 0.8 | 1.0 | 0.8 |
| **min_child_weight** | 3 | 3 | 4 | 3 |
| **üéØ CATBOOST PARAMETERS** | | | | | |
| **iterations** | 800 | 800 | 600 | 800 |
| **depth** | 10 | 10 | 8 | 10 |
| **learning_rate** | 0.015 | 0.015 | 0.02 | 0.015 |
| **l2_leaf_reg** | 0.5 | 0.5 | 1.0 | 0.5 |
| **üéØ RANDOM FOREST PARAMETERS** | | | | | |
| **n_estimators** | 800 | 800 | 600 | 800 |
| **max_depth** | 25 | 25 | 20 | 25 |
| **min_samples_split** | 6 | 6 | 8 | 6 |
| **min_samples_leaf** | 2 | 2 | 3 | 2 |
| **üéØ NEURAL NETWORK PARAMETERS** | | | | | |
| **hidden_layers** | (256, 128, 64) | (256, 128, 64) | (256, 128, 64) | (256, 128, 64) |
| **learning_rate** | adaptive | adaptive | adaptive | adaptive |
| **max_iter** | 500 | 500 | 500 | 500 |
| **batch_size** | 32 | 32 | 32 | 32 |
| **alpha** | 0.001 | 0.001 | 0.001 | 0.001 |
| **üéØ CROSS-VALIDATION & FEATURE SELECTION** | | | | | |
| **TimeSeriesSplit folds** | 7 | 7 | 5 | 7 |
| **Feature Selection (Main)** | 85% | 85% | 75% | 85% |
| **Feature Selection (RF)** | 85% | 85% | 75% | 85% |
| **Feature Selection (Method)** | 85% | 85% | 75% | 85% |
| **üéØ META-LEARNER PARAMETERS** | | | | | |
| **XGBoost Meta n_estimators** | 200 | 200 | 200 | 200 |
| **XGBoost Meta max_depth** | 4 | 4 | 4 | 4 |
| **XGBoost Meta learning_rate** | 0.05 | 0.05 | 0.05 | 0.05 |
| **LightGBM Meta n_estimators** | 200 | 200 | 200 | 200 |
| **LightGBM Meta max_depth** | 4 | 4 | 4 | 4 |
| **LightGBM Meta learning_rate** | 0.05 | 0.05 | 0.05 | 0.05 |
| **Neural Network Meta layers** | (64, 32) | (64, 32) | (64, 32) | (64, 32) |
| **Neural Network Meta max_iter** | 300 | 300 | 300 | 300 |
| **üéØ METHOD PREDICTION PARAMETERS** | | | | | |
| **XGBoost Method n_estimators** | 800 | 800 | 600 | 800 |
| **XGBoost Method max_depth** | 10 | 10 | 8 | 10 |
| **XGBoost Method learning_rate** | 0.015 | 0.015 | 0.025 | 0.015 |
| **LightGBM Method n_estimators** | 800 | 800 | 600 | 800 |
| **LightGBM Method max_depth** | 10 | 10 | 8 | 10 |
| **LightGBM Method learning_rate** | 0.015 | 0.015 | 0.025 | 0.015 |
| **Random Forest Method n_estimators** | 800 | 800 | 600 | 800 |
| **Random Forest Method max_depth** | 25 | 25 | 20 | 25 |
| **Neural Network Method layers** | (128, 64, 32) | (128, 64, 32) | (128, 64, 32) | (128, 64, 32) |
| **Neural Network Method max_iter** | 400 | 400 | 400 | 400 |

## **üìã DETAILED PARAMETER ANALYSIS**

### **üöÄ PERFORMANCE ENHANCEMENTS**

| **Model** | **Key Improvements** | **Performance Impact** |
|-----------|----------------------|------------------------|
| **exe Base** | Enhanced parameters | High accuracy, no bias correction |
| **Two Runs** | Enhanced parameters, bias correction | Higher accuracy, better generalization |
| **Class Weighting** | Baseline parameters, class weighting | Bias correction with standard performance |
| **Data Augmentation** | +67% estimators (GPU), +50% depth (GPU), -40% learning rate (GPU) | Maximum accuracy, GPU acceleration |

### **üéØ REGULARIZATION COMPARISON**

| **Model** | **XGBoost reg_alpha** | **XGBoost reg_lambda** | **LightGBM reg_alpha** | **LightGBM reg_lambda** | **CatBoost l2_leaf_reg** |
|-----------|----------------------|------------------------|------------------------|-------------------------|--------------------------|
| **exe Base** | 0.1 | 0.8 | 0.1 | 0.8 | 0.5 |
| **Two Runs** | 0.1 | 0.8 | 0.1 | 0.8 | 0.5 |
| **Class Weighting** | 0.2 | 1.0 | 0.2 | 1.0 | 1.0 |
| **Data Augmentation** | 0.1 | 0.8 | 0.1 | 0.8 | 0.5 |

### **üîç FEATURE SELECTION EVOLUTION**

| **Model** | **Main Models** | **Random Forest** | **Method Models** | **Cross-Validation** |
|-----------|-----------------|-------------------|-------------------|---------------------|
| **exe Base** | 85% | 85% | 85% | 7 folds |
| **Two Runs** | 85% | 85% | 85% | 7 folds |
| **Class Weighting** | 75% | 75% | 75% | 5 folds |
| **Data Augmentation** | 85% | 85% | 85% | 7 folds |

## **üèÜ PARAMETER OPTIMIZATION RANKING**

1. **ü•á Data Augmentation** - Enhanced parameters, balanced regularization, data augmentation
2. **ü•à Two Runs** - Enhanced parameters with bias correction
3. **ü•â exe Base** - Enhanced parameters, no bias correction
4. **4th Class Weighting** - Baseline parameters with class weighting bias correction

## **üí° KEY INSIGHTS**

- **Data Augmentation uses enhanced parameters** with data augmentation bias correction
- **Two Runs uses enhanced parameters** with bias correction
- **exe Base now uses enhanced parameters** with no bias correction
- **Class Weighting uses baseline parameters** with class weighting bias correction
- **All models use identical meta-learner parameters** for consistency
- **Feature selection varies by model**: 75% (Class Weighting) vs 85% (exe Base, Two Runs, Data Augmentation)
- **Cross-validation folds vary by model**: 5 folds (Class Weighting) vs 7 folds (exe Base, Two Runs, Data Augmentation)
